# First high-performance data pipelines in Rust

XÃ¢y dá»±ng data pipeline hiá»‡u nÄƒng cao lÃ  má»™t trong nhá»¯ng use case phá»• biáº¿n cá»§a Rust trong lÄ©nh vá»±c data engineering. Vá»›i tá»‘c Ä‘á»™ gáº§n nhÆ° C/C++, memory safety, vÃ  há»‡ sinh thÃ¡i phong phÃº, Rust lÃ  lá»±a chá»n tuyá»‡t vá»i Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c data pipeline xá»­ lÃ½ lÆ°á»£ng lá»›n dá»¯ liá»‡u.

Trong bÃ i nÃ y, chÃºng ta sáº½ xÃ¢y dá»±ng má»™t data pipeline Ä‘Æ¡n giáº£n nhÆ°ng máº¡nh máº½ Ä‘á»ƒ:
1. Äá»c dá»¯ liá»‡u tá»« CSV file
2. Xá»­ lÃ½ vÃ  transform dá»¯ liá»‡u
3. Ghi káº¿t quáº£ ra Parquet format (columnar storage hiá»‡u nÄƒng cao)

## CÃ i Ä‘áº·t dependencies

Táº¡o project má»›i:

```bash
cargo new data-pipeline
cd data-pipeline
```

ThÃªm dependencies vÃ o `Cargo.toml`:

```toml
[dependencies]
polars = { version = "0.43", features = ["lazy", "parquet", "csv"] }
anyhow = "1.0"
```

- [`polars`](../crates/polars.md): DataFrame library hiá»‡u nÄƒng cao, tÆ°Æ¡ng tá»± nhÆ° Pandas
- `anyhow`: Error handling Ä‘Æ¡n giáº£n vÃ  tiá»‡n lá»£i

## VÃ­ dá»¥ 1: Pipeline cÆ¡ báº£n

Táº¡o file `src/main.rs`:

```rust
use polars::prelude::*;
use anyhow::Result;

fn main() -> Result<()> {
    // Äá»c CSV file
    let df = CsvReader::from_path("sales_data.csv")?
        .has_header(true)
        .finish()?;

    println!("Dá»¯ liá»‡u gá»‘c:");
    println!("{:?}", df);

    // Transform: tÃ­nh tá»•ng revenue theo category
    let result = df
        .lazy()
        .group_by([col("category")])
        .agg([
            col("revenue").sum().alias("total_revenue"),
            col("quantity").sum().alias("total_quantity"),
            col("revenue").mean().alias("avg_revenue"),
        ])
        .sort("total_revenue", Default::default())
        .collect()?;

    println!("\nKáº¿t quáº£ sau khi xá»­ lÃ½:");
    println!("{:?}", result);

    // Ghi ra Parquet file
    let mut file = std::fs::File::create("output.parquet")?;
    ParquetWriter::new(&mut file).finish(&mut result.clone())?;

    println!("\nâœ… ÄÃ£ ghi káº¿t quáº£ vÃ o output.parquet");

    Ok(())
}
```

Táº¡o file dá»¯ liá»‡u máº«u `sales_data.csv`:

```csv
category,product,revenue,quantity
Electronics,Laptop,1200,2
Electronics,Mouse,25,10
Books,Novel,15,5
Books,Textbook,80,3
Electronics,Keyboard,75,4
Books,Magazine,5,20
```

Cháº¡y pipeline:

```bash
cargo run
```

## VÃ­ dá»¥ 2: Pipeline vá»›i Lazy Evaluation

Lazy evaluation giÃºp tá»‘i Æ°u performance báº±ng cÃ¡ch chá»‰ thá»±c hiá»‡n computation khi cáº§n thiáº¿t:

```rust
use polars::prelude::*;
use anyhow::Result;

fn process_large_dataset() -> Result<()> {
    let lazy_df = LazyCsvReader::new("large_dataset.csv")
        .has_header(true)
        .finish()?;

    // Äá»‹nh nghÄ©a pipeline (chÆ°a thá»±c thi)
    let result = lazy_df
        .filter(col("revenue").gt(100))  // Lá»c revenue > 100
        .select([
            col("date"),
            col("category"),
            col("revenue"),
            (col("revenue") * lit(0.1)).alias("tax"),  // TÃ­nh 10% tax
            (col("revenue") * lit(0.9)).alias("net_revenue"),
        ])
        .group_by([col("category")])
        .agg([
            col("net_revenue").sum().alias("total_net_revenue"),
            col("revenue").count().alias("transaction_count"),
        ])
        .sort("total_net_revenue", SortOptions::default().with_order_descending(true))
        .collect()?;  // Thá»±c thi táº¡i Ä‘Ã¢y

    println!("{:?}", result);
    Ok(())
}

fn main() -> Result<()> {
    process_large_dataset()?;
    Ok(())
}
```

**Æ¯u Ä‘iá»ƒm cá»§a lazy evaluation:**
- Polars tá»± Ä‘á»™ng tá»‘i Æ°u query plan
- Chá»‰ Ä‘á»c columns cáº§n thiáº¿t
- CÃ³ thá»ƒ parallel processing tá»± Ä‘á»™ng
- Giáº£m memory usage

## VÃ­ dá»¥ 3: Pipeline vá»›i Error Handling

Trong production, error handling ráº¥t quan trá»ng:

```rust
use polars::prelude::*;
use anyhow::{Context, Result};

fn read_and_validate_data(path: &str) -> Result<DataFrame> {
    let df = CsvReader::from_path(path)
        .context(format!("KhÃ´ng thá»ƒ Ä‘á»c file: {}", path))?
        .has_header(true)
        .finish()
        .context("Lá»—i parse CSV")?;

    // Validate schema
    if !df.get_column_names().contains(&"revenue") {
        anyhow::bail!("Thiáº¿u column 'revenue'");
    }

    Ok(df)
}

fn transform_data(df: DataFrame) -> Result<DataFrame> {
    df.lazy()
        .select([
            col("category"),
            col("revenue"),
            col("quantity"),
        ])
        .filter(col("revenue").gt(0))  // Lá»c bá» invalid data
        .filter(col("quantity").gt(0))
        .collect()
        .context("Lá»—i transform data")
}

fn save_to_parquet(df: &mut DataFrame, path: &str) -> Result<()> {
    let file = std::fs::File::create(path)
        .context(format!("KhÃ´ng thá»ƒ táº¡o file: {}", path))?;

    ParquetWriter::new(&file)
        .finish(df)
        .context("Lá»—i ghi Parquet file")?;

    Ok(())
}

fn main() -> Result<()> {
    println!("ğŸš€ Báº¯t Ä‘áº§u data pipeline...");

    let df = read_and_validate_data("sales_data.csv")?;
    println!("âœ… Äá»c dá»¯ liá»‡u thÃ nh cÃ´ng: {} rows", df.height());

    let mut result = transform_data(df)?;
    println!("âœ… Transform thÃ nh cÃ´ng: {} rows", result.height());

    save_to_parquet(&mut result, "output.parquet")?;
    println!("âœ… ÄÃ£ lÆ°u vÃ o output.parquet");

    Ok(())
}
```

## VÃ­ dá»¥ 4: Parallel Processing vá»›i Rayon

Xá»­ lÃ½ nhiá»u files cÃ¹ng lÃºc:

```rust
use polars::prelude::*;
use rayon::prelude::*;
use anyhow::Result;
use std::path::PathBuf;

fn process_file(path: PathBuf) -> Result<DataFrame> {
    let df = CsvReader::from_path(&path)?
        .has_header(true)
        .finish()?;

    let result = df
        .lazy()
        .group_by([col("category")])
        .agg([col("revenue").sum()])
        .collect()?;

    Ok(result)
}

fn main() -> Result<()> {
    let files = vec![
        PathBuf::from("data1.csv"),
        PathBuf::from("data2.csv"),
        PathBuf::from("data3.csv"),
    ];

    // Xá»­ lÃ½ parallel
    let results: Vec<Result<DataFrame>> = files
        .into_par_iter()
        .map(process_file)
        .collect();

    // Káº¿t há»£p káº¿t quáº£
    let mut combined = DataFrame::default();
    for result in results {
        match result {
            Ok(df) => {
                if combined.is_empty() {
                    combined = df;
                } else {
                    combined = combined.vstack(&df)?;
                }
            }
            Err(e) => eprintln!("Lá»—i xá»­ lÃ½ file: {}", e),
        }
    }

    println!("Tá»•ng há»£p: {:?}", combined);
    Ok(())
}
```

## So sÃ¡nh vá»›i Python

Pipeline tÆ°Æ¡ng tá»± trong Python vá»›i Pandas:

```python
import pandas as pd

# Äá»c CSV
df = pd.read_csv('sales_data.csv')

# Transform
result = df.groupby('category').agg({
    'revenue': ['sum', 'mean'],
    'quantity': 'sum'
}).reset_index()

# Ghi Parquet
result.to_parquet('output.parquet')
```

**Æ¯u Ä‘iá»ƒm cá»§a Rust + Polars:**
- Nhanh hÆ¡n 5-10x so vá»›i Pandas
- Memory efficient hÆ¡n nhiá»u
- Type safety táº¡i compile time
- KhÃ´ng cáº§n GIL (Global Interpreter Lock) nhÆ° Python
- Binary nhá» gá»n, dá»… deploy

**NhÆ°á»£c Ä‘iá»ƒm:**
- Compile time lÃ¢u hÆ¡n
- Ecosystem nhá» hÆ¡n Python (nhÆ°ng Ä‘ang phÃ¡t triá»ƒn nhanh)
- Learning curve cao hÆ¡n

## Best Practices

1. **Sá»­ dá»¥ng Lazy Evaluation khi lÃ m viá»‡c vá»›i large datasets:**
   ```rust
   let result = LazyCsvReader::new("large.csv")
       .finish()?
       .filter(...)
       .select(...)
       .collect()?;
   ```

2. **Streaming cho data quÃ¡ lá»›n:**
   ```rust
   let reader = CsvReader::from_path("huge.csv")?
       .has_header(true)
       .with_chunk_size(10_000);  // Äá»c tá»«ng chunk
   ```

3. **Sá»­ dá»¥ng Parquet thay vÃ¬ CSV cho storage:**
   - Nhanh hÆ¡n nhiá»u khi Ä‘á»c
   - Tiáº¿t kiá»‡m storage (compressed)
   - Preserve data types

4. **Error handling vá»›i `anyhow` hoáº·c `thiserror`:**
   ```rust
   fn process() -> Result<()> {
       let df = read_csv("data.csv")
           .context("Failed to read CSV")?;
       Ok(())
   }
   ```

## Monitoring vÃ  Logging

ThÃªm logging Ä‘á»ƒ theo dÃµi pipeline:

```rust
use polars::prelude::*;
use anyhow::Result;

fn main() -> Result<()> {
    let start = std::time::Instant::now();

    let df = CsvReader::from_path("data.csv")?
        .has_header(true)
        .finish()?;

    println!("â±ï¸  Äá»c CSV: {:?}", start.elapsed());

    let transform_start = std::time::Instant::now();
    let result = df.lazy()
        .group_by([col("category")])
        .agg([col("revenue").sum()])
        .collect()?;

    println!("â±ï¸  Transform: {:?}", transform_start.elapsed());
    println!("ğŸ“Š Sá»‘ rows: {}", result.height());

    Ok(())
}
```

## Tá»•ng káº¿t

Rust + Polars lÃ  má»™t stack máº¡nh máº½ Ä‘á»ƒ xÃ¢y dá»±ng data pipeline:
- âœ… Performance cao
- âœ… Memory safe
- âœ… Dá»… deploy (single binary)
- âœ… Ecosystem Ä‘ang phÃ¡t triá»ƒn máº¡nh

Báº¯t Ä‘áº§u vá»›i cÃ¡c pipeline Ä‘Æ¡n giáº£n, sau Ä‘Ã³ má»Ÿ rá»™ng vá»›i streaming, parallel processing, vÃ  distributed computing vá»›i cÃ¡c tools nhÆ° DataFusion vÃ  Ballista.

## References

- [Polars Documentation](https://docs.pola.rs/)
- [Polars User Guide](https://docs.pola.rs/user-guide/)
- [Are We Data Yet?](https://arewedatayet.com)
- [Apache Arrow](https://arrow.apache.org/)
- [DataFusion](https://datafusion.apache.org/)
