# Building scalable data-driven applications using Rust

Data-driven applications c·∫ßn x·ª≠ l√Ω, ph√¢n t√≠ch v√† serve d·ªØ li·ªáu m·ªôt c√°ch nhanh ch√≥ng v√† ƒë√°ng tin c·∫≠y. Rust v·ªõi performance cao, memory safety v√† concurrency m·∫°nh m·∫Ω l√† l·ª±a ch·ªçn tuy·ªát v·ªùi ƒë·ªÉ x√¢y d·ª±ng c√°c ·ª©ng d·ª•ng data-driven c√≥ kh·∫£ nƒÉng scale.

Trong b√†i n√†y, ch√∫ng ta s·∫Ω t√¨m hi·ªÉu c√°ch x√¢y d·ª±ng m·ªôt ·ª©ng d·ª•ng data-driven ho√†n ch·ªânh v·ªõi c√°c th√†nh ph·∫ßn:
- Data ingestion v√† processing
- Storage v√† indexing
- Query engine
- API ƒë·ªÉ serve data

## Ki·∫øn tr√∫c ·ª©ng d·ª•ng Data-Driven

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Data Sources‚îÇ (CSV, JSON, API, Database)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Ingestion  ‚îÇ (tokio, reqwest, async-std)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Processing  ‚îÇ (polars, arrow, datafusion)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Storage   ‚îÇ (parquet, sled, rocksdb)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Query Engine‚îÇ (datafusion, tantivy)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  API Server ‚îÇ (actix-web, axum, rocket)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## V√≠ d·ª• 1: Data Ingestion v·ªõi Tokio

X√¢y d·ª±ng service ƒë·ªÉ ingest data t·ª´ nhi·ªÅu ngu·ªìn:

```rust
use tokio::fs::File;
use tokio::io::AsyncReadExt;
use reqwest;
use anyhow::Result;

/// ƒê·ªçc data t·ª´ file
async fn ingest_from_file(path: &str) -> Result<Vec<u8>> {
    let mut file = File::open(path).await?;
    let mut contents = Vec::new();
    file.read_to_end(&mut contents).await?;
    Ok(contents)
}

/// Fetch data t·ª´ HTTP API
async fn ingest_from_api(url: &str) -> Result<String> {
    let response = reqwest::get(url).await?;
    let body = response.text().await?;
    Ok(body)
}

/// Ingest data t·ª´ nhi·ªÅu ngu·ªìn ƒë·ªìng th·ªùi
async fn ingest_all() -> Result<()> {
    // Ch·∫°y parallel
    let (file_data, api_data) = tokio::join!(
        ingest_from_file("data.csv"),
        ingest_from_api("https://api.example.com/data")
    );

    println!("File data size: {}", file_data?.len());
    println!("API data: {}", api_data?);

    Ok(())
}

#[tokio::main]
async fn main() -> Result<()> {
    ingest_all().await?;
    Ok(())
}
```

## V√≠ d·ª• 2: Processing v·ªõi DataFusion

DataFusion l√† SQL query engine s·ª≠ d·ª•ng Apache Arrow, c·ª±c k·ª≥ nhanh:

```toml
[dependencies]
datafusion = "43.0"
tokio = { version = "1.48", features = ["full"] }
```

```rust
use datafusion::prelude::*;
use anyhow::Result;

#[tokio::main]
async fn main() -> Result<()> {
    // T·∫°o SessionContext
    let ctx = SessionContext::new();

    // ƒêƒÉng k√Ω Parquet file nh∆∞ m·ªôt table
    ctx.register_parquet("sales", "sales_data.parquet", ParquetReadOptions::default())
        .await?;

    // Ch·∫°y SQL query
    let df = ctx.sql(
        "SELECT
            category,
            SUM(revenue) as total_revenue,
            AVG(revenue) as avg_revenue,
            COUNT(*) as num_transactions
        FROM sales
        WHERE revenue > 100
        GROUP BY category
        ORDER BY total_revenue DESC"
    ).await?;

    // Hi·ªÉn th·ªã k·∫øt qu·∫£
    df.show().await?;

    // Ho·∫∑c l∆∞u l·∫°i th√†nh Parquet
    df.write_parquet("output.parquet", None, None).await?;

    Ok(())
}
```

**∆Øu ƒëi·ªÉm c·ªßa DataFusion:**
- T·ªëc ƒë·ªô c·ª±c nhanh (columnar processing)
- H·ªó tr·ª£ SQL standard
- C√≥ th·ªÉ scale ƒë·∫øn h√†ng TB d·ªØ li·ªáu
- Zero-copy reads v·ªõi Arrow

## V√≠ d·ª• 3: Storage v·ªõi Parquet

Parquet l√† format columnar storage r·∫•t hi·ªáu qu·∫£:

```rust
use polars::prelude::*;
use anyhow::Result;

fn save_to_parquet(df: &mut DataFrame, path: &str) -> Result<()> {
    let file = std::fs::File::create(path)?;

    ParquetWriter::new(&file)
        .with_compression(ParquetCompression::Snappy)  // N√©n data
        .finish(df)?;

    Ok(())
}

fn read_from_parquet(path: &str) -> Result<DataFrame> {
    let df = ParquetReader::new(std::fs::File::open(path)?)
        .finish()?;
    Ok(df)
}

fn main() -> Result<()> {
    // T·∫°o sample data
    let mut df = df! {
        "id" => &[1, 2, 3, 4, 5],
        "name" => &["Alice", "Bob", "Charlie", "David", "Eve"],
        "revenue" => &[1200.50, 800.30, 1500.75, 950.20, 1100.00],
    }?;

    // L∆∞u
    save_to_parquet(&mut df, "users.parquet")?;
    println!("‚úÖ ƒê√£ l∆∞u data");

    // ƒê·ªçc l·∫°i
    let loaded = read_from_parquet("users.parquet")?;
    println!("{:?}", loaded);

    Ok(())
}
```

## V√≠ d·ª• 4: Full-text Search v·ªõi Tantivy

Tantivy l√† full-text search engine t∆∞∆°ng t·ª± Lucene/Elasticsearch:

```toml
[dependencies]
tantivy = "0.22"
```

```rust
use tantivy::schema::*;
use tantivy::{doc, Index, IndexWriter, ReloadPolicy};
use tantivy::collector::TopDocs;
use tantivy::query::QueryParser;
use anyhow::Result;

fn main() -> Result<()> {
    // ƒê·ªãnh nghƒ©a schema
    let mut schema_builder = Schema::builder();
    schema_builder.add_text_field("title", TEXT | STORED);
    schema_builder.add_text_field("body", TEXT);
    let schema = schema_builder.build();

    // T·∫°o index
    let index = Index::create_in_ram(schema.clone());
    let mut index_writer: IndexWriter = index.writer(50_000_000)?;

    // Index documents
    let title = schema.get_field("title").unwrap();
    let body = schema.get_field("body").unwrap();

    index_writer.add_document(doc!(
        title => "Rust Programming",
        body => "Rust is a systems programming language that runs blazingly fast"
    ))?;

    index_writer.add_document(doc!(
        title => "Data Engineering",
        body => "Building scalable data pipelines with Rust and Polars"
    ))?;

    index_writer.commit()?;

    // Search
    let reader = index
        .reader_builder()
        .reload_policy(ReloadPolicy::OnCommitWithDelay)
        .try_into()?;

    let searcher = reader.searcher();

    let query_parser = QueryParser::for_index(&index, vec![title, body]);
    let query = query_parser.parse_query("Rust")?;

    let top_docs = searcher.search(&query, &TopDocs::with_limit(10))?;

    println!("T√¨m th·∫•y {} k·∫øt qu·∫£:", top_docs.len());
    for (_score, doc_address) in top_docs {
        let retrieved_doc = searcher.doc(doc_address)?;
        println!("{}", schema.to_json(&retrieved_doc));
    }

    Ok(())
}
```

## V√≠ d·ª• 5: API Server v·ªõi Axum

X√¢y d·ª±ng REST API ƒë·ªÉ serve data:

```toml
[dependencies]
axum = "0.7"
tokio = { version = "1.48", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
datafusion = "43.0"
```

```rust
use axum::{
    extract::{Query, State},
    http::StatusCode,
    routing::get,
    Json, Router,
};
use datafusion::prelude::*;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use anyhow::Result;

#[derive(Clone)]
struct AppState {
    ctx: Arc<SessionContext>,
}

#[derive(Deserialize)]
struct QueryParams {
    category: Option<String>,
    min_revenue: Option<f64>,
}

#[derive(Serialize)]
struct SalesRecord {
    category: String,
    total_revenue: f64,
    num_transactions: i64,
}

async fn query_sales(
    State(state): State<AppState>,
    Query(params): Query<QueryParams>,
) -> Result<Json<Vec<SalesRecord>>, StatusCode> {
    // Build SQL query d·ª±a tr√™n params
    let mut sql = String::from(
        "SELECT category,
         SUM(revenue) as total_revenue,
         COUNT(*) as num_transactions
         FROM sales WHERE 1=1"
    );

    if let Some(cat) = params.category {
        sql.push_str(&format!(" AND category = '{}'", cat));
    }

    if let Some(min_rev) = params.min_revenue {
        sql.push_str(&format!(" AND revenue >= {}", min_rev));
    }

    sql.push_str(" GROUP BY category");

    // Execute query
    let df = state.ctx.sql(&sql)
        .await
        .map_err(|_| StatusCode::INTERNAL_SERVER_ERROR)?;

    // Convert to JSON (simplified)
    let records = vec![
        SalesRecord {
            category: "Electronics".to_string(),
            total_revenue: 5000.0,
            num_transactions: 100,
        }
    ];

    Ok(Json(records))
}

async fn health_check() -> &'static str {
    "OK"
}

#[tokio::main]
async fn main() -> Result<()> {
    // Setup DataFusion
    let ctx = SessionContext::new();
    ctx.register_parquet("sales", "sales_data.parquet", ParquetReadOptions::default())
        .await?;

    let state = AppState {
        ctx: Arc::new(ctx),
    };

    // Build router
    let app = Router::new()
        .route("/health", get(health_check))
        .route("/api/sales", get(query_sales))
        .with_state(state);

    // Run server
    println!("üöÄ Server ch·∫°y t·∫°i http://localhost:3000");
    let listener = tokio::net::TcpListener::bind("0.0.0.0:3000").await?;
    axum::serve(listener, app).await?;

    Ok(())
}
```

Test API:

```bash
# Health check
curl http://localhost:3000/health

# Query v·ªõi params
curl "http://localhost:3000/api/sales?category=Electronics&min_revenue=100"
```

## V√≠ d·ª• 6: Real-time Processing v·ªõi Channels

X·ª≠ l√Ω data streaming real-time:

```rust
use tokio::sync::mpsc;
use tokio::time::{Duration, sleep};
use anyhow::Result;

#[derive(Debug, Clone)]
struct DataPoint {
    timestamp: i64,
    value: f64,
}

async fn producer(tx: mpsc::Sender<DataPoint>) {
    let mut counter = 0;
    loop {
        let data = DataPoint {
            timestamp: counter,
            value: (counter as f64) * 1.5,
        };

        if tx.send(data).await.is_err() {
            println!("Receiver dropped");
            break;
        }

        counter += 1;
        sleep(Duration::from_millis(100)).await;
    }
}

async fn processor(mut rx: mpsc::Receiver<DataPoint>) {
    let mut buffer = Vec::new();
    let batch_size = 10;

    while let Some(data) = rx.recv().await {
        buffer.push(data);

        if buffer.len() >= batch_size {
            // Process batch
            let sum: f64 = buffer.iter().map(|d| d.value).sum();
            let avg = sum / buffer.len() as f64;

            println!("üìä Batch processed: {} records, avg = {:.2}", buffer.len(), avg);

            buffer.clear();
        }
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    let (tx, rx) = mpsc::channel(100);

    // Spawn producer v√† processor
    let producer_handle = tokio::spawn(producer(tx));
    let processor_handle = tokio::spawn(processor(rx));

    // Ch·∫°y 5 gi√¢y
    sleep(Duration::from_secs(5)).await;
    producer_handle.abort();

    processor_handle.await?;

    Ok(())
}
```

## Best Practices

### 1. S·ª≠ d·ª•ng Columnar Format

```rust
// ‚úÖ T·ªët: Parquet (columnar)
ParquetWriter::new(&file).finish(&mut df)?;

// ‚ùå Tr√°nh: CSV cho large datasets
// CSV r·∫•t ch·∫≠m v√† t·ªën memory
```

### 2. Lazy Evaluation

```rust
// ‚úÖ T·ªët: Lazy evaluation
let result = df.lazy()
    .filter(...)
    .select(...)
    .collect()?;  // Ch·ªâ execute 1 l·∫ßn

// ‚ùå Tr√°nh: Eager evaluation
let df = df.filter(...)?;  // Execute ngay
let df = df.select(...)?;  // Execute l·∫°i
```

### 3. Batch Processing

```rust
// ‚úÖ T·ªët: Process theo batch
for chunk in data.chunks(10_000) {
    process_chunk(chunk);
}

// ‚ùå Tr√°nh: Process t·ª´ng item
for item in data {
    process_item(item);  // Ch·∫≠m!
}
```

### 4. Connection Pooling

```rust
use deadpool_postgres::{Config, Pool};

// ‚úÖ T·ªët: D√πng connection pool
let pool = config.create_pool(None, NoTls)?;
let client = pool.get().await?;

// ‚ùå Tr√°nh: T·∫°o connection m·ªói l·∫ßn query
```

## Monitoring v√† Observability

```rust
use std::time::Instant;

fn main() -> Result<()> {
    let start = Instant::now();

    // Process data
    let df = process_data()?;

    println!("‚è±Ô∏è  Processing time: {:?}", start.elapsed());
    println!("üìä Records processed: {}", df.height());
    println!("üíæ Memory used: {} MB", df.estimated_size() / 1_000_000);

    Ok(())
}
```

## So s√°nh Performance

| Operation | Python/Pandas | Rust/Polars | Speedup |
|-----------|---------------|-------------|---------|
| Read CSV (1GB) | 5.2s | 0.8s | 6.5x |
| GroupBy + Agg | 3.1s | 0.4s | 7.8x |
| Join (10M rows) | 12.5s | 1.2s | 10.4x |
| Write Parquet | 2.8s | 0.5s | 5.6x |

## T·ªïng k·∫øt

Rust cung c·∫•p ecosystem m·∫°nh m·∫Ω ƒë·ªÉ x√¢y d·ª±ng data-driven applications:
- ‚úÖ **DataFusion**: SQL query engine c·ª±c nhanh
- ‚úÖ **Polars**: DataFrame library nh∆∞ Pandas nh∆∞ng nhanh h∆°n nhi·ªÅu
- ‚úÖ **Tantivy**: Full-text search engine
- ‚úÖ **Arrow**: Columnar format chu·∫©n c√¥ng nghi·ªáp
- ‚úÖ **Tokio**: Async runtime cho concurrent processing
- ‚úÖ **Axum/Actix**: Web frameworks ƒë·ªÉ serve data

V·ªõi Rust, b·∫°n c√≥ th·ªÉ x√¢y d·ª±ng ·ª©ng d·ª•ng v·ª´a nhanh, v·ª´a safe, v·ª´a d·ªÖ maintain v√† scale.

## References

- [DataFusion Documentation](https://datafusion.apache.org/)
- [Polars Documentation](https://docs.pola.rs/)
- [Tantivy Documentation](https://docs.rs/tantivy/)
- [Apache Arrow](https://arrow.apache.org/)
- [Are We Data Yet?](https://arewedatayet.com)
